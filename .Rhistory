urlgetter <- function(x){
str <- gsub(" ", "+", x)
togc <- paste0("http://www.datasciencetoolkit.org/maps/api/geocode/json?sensor=false&address=", str)
return(togc)
}
#This sends the urls to the Open Data Science Toolkit geocoder and returns lat, lon, address, and type
geocoder <- function(x){
return <- GET(x) %>% read_html %>% html_text %>% fromJSON()
if (return$status == "OK"){
results <- as.data.frame(cbind(return$results$geometry$location$lat, return$results$geometry$location$lng, paste0(return$results$address_components[[1]]$short_name, sep = " ", collapse = ""), return$results$geometry$location_typ))
colnames(goods) <- c("Lat", "Lon", "Address", "Type")
} else {
results <- "No Results"
}
return(results)
}
urls <- lapply(x, urlgetter)
df <- lapply(urls, geocoder)
return(df)
}
shityeah <- alsgeocoder(addresses)
alsgeocoder <- function(x){
#This creates urls from the input addresses
urlgetter <- function(x){
str <- gsub(" ", "+", x)
togc <- paste0("http://www.datasciencetoolkit.org/maps/api/geocode/json?sensor=false&address=", str)
return(togc)
}
#This sends the urls to the Open Data Science Toolkit geocoder and returns lat, lon, address, and type
geocoder <- function(x){
return <- GET(x) %>% read_html %>% html_text %>% fromJSON()
if (return$status == "OK"){
results <- as.data.frame(cbind(return$results$geometry$location$lat, return$results$geometry$location$lng, paste0(return$results$address_components[[1]]$short_name, sep = " ", collapse = ""), return$results$geometry$location_typ))
colnames(results) <- c("Lat", "Lon", "Address", "Type")
} else {
results <- "No Results"
}
return(results)
}
urls <- lapply(x, urlgetter)
df <- lapply(urls, geocoder)
return(df)
}
shityeah <- alsgeocoder(addresses)
alsgeocoder <- function(x){
#This creates urls from the input addresses
urlgetter <- function(x){
str <- gsub(" ", "+", x)
togc <- paste0("http://www.datasciencetoolkit.org/maps/api/geocode/json?sensor=false&address=", str)
return(togc)
}
#This sends the urls to the Open Data Science Toolkit geocoder and returns lat, lon, address, and type
geocoder <- function(x){
return <- x %>% read_html %>% html_text %>% fromJSON()
if (return$status == "OK"){
results <- as.data.frame(cbind(return$results$geometry$location$lat, return$results$geometry$location$lng, paste0(return$results$address_components[[1]]$short_name, sep = " ", collapse = ""), return$results$geometry$location_typ))
colnames(results) <- c("Lat", "Lon", "Address", "Type")
} else {
results <- "No Results"
}
return(results)
}
urls <- lapply(x, urlgetter)
df <- lapply(urls, geocoder)
return(df)
}
shityeah <- alsgeocoder(addresses)
View(shityeah)
rbind(shityeah)
lapply(shityeah, rbind)
chkers <- lapply(shityeah, rbind)
lapply
View(chkers)
shityeah <- alsgeocoder(addresses)
alsgeocoder <- function(x){
#This creates urls from the input addresses
urlgetter <- function(x){
str <- gsub(" ", "+", x)
togc <- paste0("http://www.datasciencetoolkit.org/maps/api/geocode/json?sensor=false&address=", str)
return(togc)
}
#This sends the urls to the Open Data Science Toolkit geocoder and returns lat, lon, address, and type
geocoder <- function(x){
return <- x %>% read_html() %>% html_text() %>% fromJSON()
if (return$status == "OK"){
results <- as.data.frame(cbind(return$results$geometry$location$lat, return$results$geometry$location$lng, paste0(return$results$address_components[[1]]$short_name, sep = " ", collapse = ""), return$results$geometry$location_typ))
colnames(results) <- c("Lat", "Lon", "Address", "Type")
} else {
results <- "No Results"
}
return(results)
}
urls <- lapply(x, urlgetter)
df <- lapply(urls, geocoder)
return(df)
}
shityeah <- alsgeocoder(addresses)
library(rvest)
library(jsonlite)
library(purrr)
library(dplyr)
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("/home/atrusty/alectrusty.github.io/")
#render your sweet site.
rmarkdown::render_site()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
test <- "https://www.yelp.com/search?cflt=nightlife&find_loc=Portland%2C+OR"
library(rvest)
library(jsonlite)
library(purrr)
library(dplyr)
library(lubridate)
library(stringr)
library(leaflet)
library(pander)
options(stringsAsFactors = F, scipen = 999)
panderOptions("round",6)
#Scrape business links
bizlinks<-unlist(lapply(test,function(x){
lin <- read_html(x) %>% html_nodes(".biz-name.js-analytics-click") %>% html_attr('href')
return(lin)
}))
#Paste business links to new url
bizurls <- unlist(lapply(bizlinks, function(x) {
paste0("https://www.yelp.com", x)
}))
#Read the html from each url to the local environment
bizhtmls <- lapply(bizurls, read_html)
textscrapr <- function(x, node){
txt <- x %>% html_nodes(node) %>% html_text()
ifelse(identical(txt, character(0)), "", txt)
}
names <- lapply(bizhtmls, textscrapr, node = ".biz-page-title")
address <- lapply(bizhtmls, textscrapr, node = ".street-address")
mon_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(1) th+ td")
tue_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(2) th+ td")
wed_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(3) th+ td")
thu_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(4) th+ td")
fri_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(5) th+ td")
sat_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(6) th+ td")
sun_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(7) th+ td")
prange <- lapply(bizhtmls, textscrapr, node = ".price-description")
tags <- lapply(bizhtmls, textscrapr, node = ".price-category")
latitude <- as.character(unlist(lapply(bizhtmls, function(x){
locjson <- x %>% html_node(".lightbox-map") %>% html_attr("data-map-state") %>% fromJSON()
lat <- locjson$center$latitude
return(lat)
})))
tags
wed_hrs
bizhtmls
bizlinks
lin <- read_html(test[1]) %>% html_nodes(".biz-name.js-analytics-click") %>% html_attr('href')
bad <- which(lapply(lin, nchar) > 60)
bad
#Scrape business links
bizlinks<-unlist(lapply(test,function(x){
lin <- read_html(x) %>% html_nodes(".biz-name.js-analytics-click") %>% html_attr('href')
bad <- which(lapply(lin, nchar) > 60) #finds urls with more than 60 characters (very likely advetisements)
lin <- lin[-bad] #removes the bad urls
return(lin)
}))
#Paste business links to new url
bizurls <- unlist(lapply(bizlinks, function(x) {
paste0("https://www.yelp.com", x)
}))
#Read the html from each url to the local environment
bizhtmls <- lapply(bizurls, read_html)
latitude <- as.character(unlist(lapply(bizhtmls, function(x){
locjson <- x %>% html_node(".lightbox-map") %>% html_attr("data-map-state") %>% fromJSON()
lat <- locjson$center$latitude
return(lat)
})))
longitude <- as.character(unlist(lapply(bizhtmls, function(x){
locjson <- x %>% html_node(".lightbox-map") %>% html_attr("data-map-state") %>% fromJSON()
lon <- locjson$center$longitude
return(lon)
})))
as.numeric(lapply(yelpdf$fri_close[1], close_num))
close_num <- function(x){
if (x == "Closed") {
return(NA)
} else if(!is.na(x)) {
first <- as.numeric(unlist(str_split(x, "\\:"))[1])
second <- unlist(str_split(x, "\\:"))[2]
second <- gsub("3", "5", second)
if (first >= 7) {
paste0(first - 6, ".", second)
} else if (first <= 6) {
paste0(first + 18, ".", second)
} else {
return(x)
}
} else {
return(x)
}
}
as.numeric(lapply(yelpdf$fri_close[1], close_num))
address <- lapply(bizhtmls, textscrapr, node = ".street-address")
mon_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(1) th+ td")
tue_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(2) th+ td")
wed_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(3) th+ td")
thu_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(4) th+ td")
fri_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(5) th+ td")
sat_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(6) th+ td")
sun_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(7) th+ td")
prange <- lapply(bizhtmls, textscrapr, node = ".price-description")
tags <- lapply(bizhtmls, textscrapr, node = ".price-category")
names <- lapply(bizhtmls, textscrapr, node = ".biz-page-title")
yelpdf <- as.data.frame(cbind(names, address, mon_hrs, tue_hrs, wed_hrs, thu_hrs,fri_hrs, sat_hrs, sun_hrs, prange, tags, latitude, longitude))
# Name Clean-Up
yelpdf$name <- gsub("\\\n|   |  ", "", yelpdf$name) #Removes "\\n, and unnecessary spaces
# Address Clean-Up
yelpdf$address <- gsub("\\\n|    |  ", "", yelpdf$address)
yelpdf$address <- gsub("Portland", " Portland", yelpdf$address)
yelpdf$address <- gsub("Ste", " Ste", yelpdf$address)
yelpdf$address <- gsub("Beaverton", " Beaverton", yelpdf$address)
# Hours Clean-Up
yelpdf$mon_hrs <- gsub("\\\n|   |  ", "", yelpdf$mon_hrs)
yelpdf$tue_hrs <- gsub("\\\n|   |  ", "", yelpdf$tue_hrs)
yelpdf$wed_hrs <- gsub("\\\n|   |  ", "", yelpdf$wed_hrs)
yelpdf$thu_hrs <- gsub("\\\n|   |  ", "", yelpdf$thu_hrs)
yelpdf$fri_hrs <- gsub("\\\n|   |  ", "", yelpdf$fri_hrs)
yelpdf$sat_hrs <- gsub("\\\n|   |  ", "", yelpdf$sat_hrs)
yelpdf$sun_hrs <- gsub("\\\n|   |  ", "", yelpdf$sun_hrs)
# Price Range Clean-Up
yelpdf$prange <- gsub("\\\n|    |  ", "", yelpdf$prange)
# Tags Clean-Up
yelpdf$tags <- gsub("\\\n|\\$|   |  |Edit|Opens|a popup|categories|category", "", yelpdf$tags)
yelpdf$tags <- gsub("\\,","|",yelpdf$tags)
yelpdf$tags <- gsub("   \\|    ", "", yelpdf$tags)
yelpdf$tags <- gsub("\\|", " | ", yelpdf$tags)
pop <- paste0("<b>Business Name:</b> ", yelpdf$name, #set the popup
"<br><b>Address:</b> ", yelpdf$address,
"<br><b>Monday Hours:</b> ", yelpdf$mon_hrs,
"<br><b>Tuesday Hours:</b> ", yelpdf$tue_hrs,
"<br><b>Wednesday Hours:</b> ", yelpdf$wed_hrs,
"<br><b>Thursday Hours:</b> ", yelpdf$thu_hrs,
"<br><b>Friday Hours:</b> ", yelpdf$fri_hrs,
"<br><b>Saturday Hours:</b> ", yelpdf$sat_hrs,
"<br><b>Price Range:</b> ", yelpdf$prange,
"<br><b>Tags:</b> ", yelpdf$tags)
leaflet() %>% addTiles() %>%  addMarkers(data = yelpdf, popup = pop,lat = as.numeric(yelpdf$latitude), lng = as.numeric(as.character(yelpdf$longitude)))
opentime <- function(x){
if (x == "Closed") {
return(x)
} else {
spl <- toupper(unlist(str_split(x, " ")))
op <- paste0(spl[1], " ", spl[2])
optime <- strptime(op, format = "%I:%M %p")
optime <- strftime(optime, format = "%H:%M")
}
}
closetime <- function(x){
if (x == "Closed") {
return(x)
} else {
spl <- toupper(unlist(str_split(x, " ")))
cl <- paste0(spl[length(spl)-1], " ", spl[length(spl)])
cltime <- strptime(cl, format = "%I:%M %p")
cltime <- strftime(cltime, format = "%H:%M")
}
}
yelpdf$fri_open <- as.character(lapply(yelpdf$fri_hrs, opentime))
yelpdf$fri_close <- as.character(lapply(yelpdf$fri_hrs, closetime))
yelpdf$sat_open <- as.character(lapply(yelpdf$sat_hrs, opentime))
yelpdf$sat_close <- as.character(lapply(yelpdf$sat_hrs, closetime))
close_num <- function(x){
if (x == "Closed") {
return(NA)
} else if(!is.na(x)) {
first <- as.numeric(unlist(str_split(x, "\\:"))[1])
second <- unlist(str_split(x, "\\:"))[2]
second <- gsub("3", "5", second)
if (first >= 7) {
paste0(first - 6, ".", second)
} else if (first <= 6) {
paste0(first + 18, ".", second)
} else {
return(x)
}
} else {
return(x)
}
}
as.numeric(lapply(yelpdf$fri_close[1], close_num))
yelpdf$fri_close
as.numeric(lapply(yelpdf$fri_close[8], close_num))
close_num <- function(x){
if (x == "Closed" | is.na(x)) {
return(NA)
} else if(!is.na(x)) {
first <- as.numeric(unlist(str_split(x, "\\:"))[1])
second <- unlist(str_split(x, "\\:"))[2]
second <- gsub("3", "5", second)
if (first >= 7) {
paste0(first - 6, ".", second)
} else if (first <= 6) {
paste0(first + 18, ".", second)
} else {
return(x)
}
} else {
return(x)
}
}
as.numeric(lapply(yelpdf$fri_close[8], close_num))
yelpdf$fri_close_num <- as.numeric(lapply(yelpdf$fri_close, close_num))
yelpdf$sat_close_num <- as.numeric(lapply(yelpdf$sat_close, close_num))
View(yelpdf)
pander(head(yelpdf))
require(rgdal)
wgs84 <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
yelpdf2$latitude <- as.numeric(as.character(yelpdf2$latitude))
yelpdf$latitude <- as.numeric(as.character(yelpdf$latitude))
yelpdf$longitude <- as.numeric(as.character(yelpdf$longitude))
names(yelpdf)
yelpsp <- SpatialPointsDataFrame(coords = yelpdf[,c(13,12)],data = yelpdf, proj4string = wgs84)
plot(yelpsp)
View(yelpdf)
test <- "https://www.yelp.com/search?cflt=nightlife&find_loc=Portland%2C+OR"
bizlinks<-unlist(lapply(test,function(x){
lin <- read_html(x) %>% html_nodes(".biz-name.js-analytics-click") %>% html_attr('href')
bad <- which(lapply(lin, nchar) > 60) #finds urls with more than 60 characters (very likely advetisements)
lin <- lin[-bad] #removes the bad urls
return(lin)
}))
#Paste business links to new url
bizurls <- unlist(lapply(bizlinks, function(x) {
paste0("https://www.yelp.com", x)
}))
#Read the html from each url to the local environment
bizhtmls <- lapply(bizurls, read_html)
textscrapr <- function(x, node){
txt <- x %>% html_nodes(node) %>% html_text()
ifelse(identical(txt, character(0)), "", txt)
}
names <- lapply(bizhtmls, textscrapr, node = ".biz-page-title")
address <- lapply(bizhtmls, textscrapr, node = ".street-address")
mon_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(1) th+ td")
tue_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(2) th+ td")
wed_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(3) th+ td")
thu_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(4) th+ td")
fri_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(5) th+ td")
sat_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(6) th+ td")
sun_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(7) th+ td")
prange <- lapply(bizhtmls, textscrapr, node = ".price-description")
tags <- lapply(bizhtmls, textscrapr, node = ".price-category")
latitude <- as.character(unlist(lapply(bizhtmls, function(x){
locjson <- x %>% html_node(".lightbox-map") %>% html_attr("data-map-state") %>% fromJSON()
lat <- locjson$center$latitude
return(lat)
})))
longitude <- as.character(unlist(lapply(bizhtmls, function(x){
locjson <- x %>% html_node(".lightbox-map") %>% html_attr("data-map-state") %>% fromJSON()
lon <- locjson$center$longitude
return(lon)
})))
yelpdf <- as.data.frame(cbind(names, address, mon_hrs, tue_hrs, wed_hrs, thu_hrs,fri_hrs, sat_hrs, sun_hrs, prange, tags, latitude, longitude))
head(yelpdf)
# Name Clean-Up
yelpdf$names <- gsub("\\\n|   |  ", "", yelpdf$name) #Removes "\\n, and unnecessary spaces
name <- lapply(bizhtmls, textscrapr, node = ".biz-page-title")
address <- lapply(bizhtmls, textscrapr, node = ".street-address")
mon_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(1) th+ td")
tue_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(2) th+ td")
wed_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(3) th+ td")
thu_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(4) th+ td")
fri_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(5) th+ td")
sat_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(6) th+ td")
sun_hrs <- lapply(bizhtmls, textscrapr, node = "tr:nth-child(7) th+ td")
prange <- lapply(bizhtmls, textscrapr, node = ".price-description")
tags <- lapply(bizhtmls, textscrapr, node = ".price-category")
latitude <- as.character(unlist(lapply(bizhtmls, function(x){
locjson <- x %>% html_node(".lightbox-map") %>% html_attr("data-map-state") %>% fromJSON()
lat <- locjson$center$latitude
return(lat)
})))
longitude <- as.character(unlist(lapply(bizhtmls, function(x){
locjson <- x %>% html_node(".lightbox-map") %>% html_attr("data-map-state") %>% fromJSON()
lon <- locjson$center$longitude
return(lon)
})))
yelpdf <- as.data.frame(cbind(name, address, mon_hrs, tue_hrs, wed_hrs, thu_hrs,fri_hrs, sat_hrs, sun_hrs, prange, tags, latitude, longitude))
name[1:2]
pander(yelpdf)
pander(yelpdf)
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("/home/atrusty/alectrusty.github.io/")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("/home/atrusty/alectrusty.github.io/")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("/home/atrusty/alectrusty.github.io/")
#render your sweet site.
rmarkdown::render_site()
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("/home/atrusty/alectrusty.github.io/")
#render your sweet site.
rmarkdown::render_site()
# Source Scripts and Data Necessary
source("/media/atrusty/WHISTLE/Winter_Term_2017/ESM567/Homework_1/cor.matrix.R")
source("/media/atrusty/WHISTLE/Winter_Term_2017/ESM567/Homework_1/broken.stick.R")
hw1 <- read.csv("/media/atrusty/WHISTLE/Winter_Term_2017/ESM567/Homework_1/wemap_pnw_rda_HW.csv")
View(hw1)
cor.matrix(hw1[,2:10])
hw1.1<-na.omit(hw1)  #remove all missing values from the dataset
wq.orig<-hw1.1[,c(2:20)]      #split the data into a subset with all water quality variables
ws.orig<-hw1.1[,c(21:34)]
wq<-hw1.1[,c(2:20)]      #split the data into a subset with all water quality variables
ws<-hw1.1[,c(21:34)]
wq2 <- scale(wq)
ws2 <- scale(ws)
cor.matrix(wq[,c(1:5)])
cor.matrix(wq[,c(5:10)])
cor.matrix(wq[,c(10:15)])
cor.matrix(wq[,c(15:19)])
boxplot(wq.orig, xlab = "Water Quality Variable", ylab = "Distribution")
boxplot(scale(log(wq)), xlab = "Water Quality Variable", ylab = "Log Distribution")
boxplot(ws.orig, xlab = "Watershed Variable", ylab = "Distribution")
boxplot(log(ws.orig))
par(mfrow = c(1,2))
boxplot(ws, xlab = "Watershed Variables", ylab = "Distribution")
boxplot(scale(log(ws)), xlab = "Watershed Variables", ylab = "Log Distribution")
boxplot(log(ws$WSAREA))
cor.matrix(log(wq))
cor.matrix(log(wq[,c(1:5)]+1))
cor.matrix(log(wq[,c(5:10)]+1))
cor.matrix(log(wq[,c(10:15)]+1))
cor.matrix(log(wq[,c(15:19)]+1)) #Turbidity STILL highly skewed to the left
#Run Variance/Covariance
round(var(scale(log(wq+1))),2)
cor.matrix(log(wq2+1))
round(var(log(wq+1)),2) #variance/covariance matrix
diag(round(var(scale(log(wq+1)),2)))   #show variance only
#2.
#Run PCA first
pca.wq<-princomp(scale(log(wq+1))) #run PCA with log-transformed and standardized data
biplot(pca.wq) #biplot
summary(pca.wq) #eigenvalues
round(loadings(pca.wq)[,c(1:2)],2)  #eigenvectors for PC1 and 2 only
broken.stick(19)
pca.wq$scores #PC.Matrix showing site scores for all PCs
### We can see that PC1 has greater prop variance (.46) than randomized (.24),
### But PC2 vlaues smaller (0.14) than randomized (0.16).  Therefor, we ONLY interpret
### PC1, but we need PC2 to plot
dta.new<-data.frame(PCI=pca.wq$scores[,c(1,2)],ws2)
PC_ws<-data.frame(PCI=pca.wq$scores[,c(1,2)],ws)
cor(dta.new, method=c("spearman"))
round(var(PC_ws[,c(3:16)],PC_ws[,c(1:2)]))
#Regress PC 1 with additional explanatory variables
#Perform an Indirect analysis: regress PC1 against 4 watershed variables
dta.new<-data.frame(PCI=pca.wq$scores[,1],ws)  #merge two datasets into one side by side
colnames(dta.new)
cor.matrix(dta.new[,c(1,2,10)])   #only show 2 explanatory variables
boxplot(dta.new)
boxplot(log(dta.new))
fullmod <- lm(PCI~., data = dta.new)
summary(fullmod)
plot(fullmod)
mod<-lm(PCI~scale(log(PRECIP_M))+scale(log(SLOPMEAN)), data=dta.new) #simple regression
summary(mod) #detailed regression model output, How much variance in PC1 is explained by the two variables?
### HW PART 3:  Run direct gradient Analysis
#1.  RDA full model (with all Xs)
require(vegan)
install.packages("vegan")
#Check VIF?
vif.cca(rda.pnw.1)
boxplot(dta.new)
boxplot(log(dta.new))
fullmod <- lm(PCI~., data = dta.new)
summary(fullmod)
plot(fullmod)
### HW PART 3:  Run direct gradient Analysis
#1.  RDA full model (with all Xs)
require(vegan)
#Full Model
rda.pnw <- rda(scale(log(wq+1))~., center = T, data= log(ws+1), scale=T)
summary(rda.pnw)
#Null Model
mod.0 <- rda(scale(log(wq+1))~1, center = T, data= log(ws+1),scale=T)
summary(mod.0)
#Hybrid Selection
mod.1 <- step(mod.0, scope = formula(rda.pnw))
#Potential Reduced Model
rda.pnw.1 <- rda(scale(log(wq + 1)) ~ PRECIP_M + ELEVMEAN + LAT_DD + AG_TOT +
LON_DD + RNG_TOT + SLOPMEAN + W1_HALL + FOR_TOT + WSAREA +
RD_DEN + POPDENKM, data = log(ws+1), center = T, scale = T)
plot(rda.pnw.1)
summary(rda.pnw.1)
#Check VIF?
vif.cca(rda.pnw.1)
par(mfrow=c(1,1))
plot(rda.pnw.1)
